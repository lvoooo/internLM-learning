# LMDeploy 的量化和部署

## 大模型部署背景

### 部署

定义:
- 将训练好的模型在特定软硬件环境中启动的过程，使模型能够接收输入并返回预测结果
- 为了满足性能和效率的需求，常常需要对模型进行优化，例如模型压缩和硬件加速

产品形态:
- 云端、边缘计算端、移动端

计算设备:
- CPU、GPU、NPU、TPU 

### 大模型特点

**内存开销巨大**
- 庞大的参数量。 7B 模型仅权重就需要 14+G 内存
- 采用`自回归生成 token`，需要缓存 Attention 的 k/v,带来巨大的内存开销

**动态shape**
- 请求数不固定
- Token 逐个生成，且数量不定

**相对视觉模型，LLM结构简单**
- Transformers 结构，大部分是 decoder-only

### 大模型部署挑战
设备：
- 如何应对巨大的存储问题？低存储设备（消费级显卡、手机等）如何部署？

推理：
- 如何加速 token 的生成速度
- 如何解决动态shape，让推理可以不间断
- 如何有效管理和利用内存

服务：
- 如何提升系统整体吞吐量？
- 对于个体用户，如何降低响应时间？

技术点：
- 模型并行
- transformer 计算和访存优化
- 低比特量化
- Continuous Batch
- Page Attention


方案
- huggingface transformers
- 专门的推理加速框架
  - 云端
    -  Imdeploy
    -  vllm
    -  tensorrt-Ilm
    -  deepspeed
  - 移动端
    - Ilama.cpp
    - mlc-Ilm

### LMDeplpoy 

英伟达设备上部署的全流程解决方案。
模型轻量化、推理、服务。


![image](https://github.com/lvoooo/internLM-learning/assets/16740247/09f2c154-b9f6-4cab-ae57-2c0316e1d85d)

推理性能

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/9de0abde-b4e2-40cf-a5cf-2d5000ea418e)

#### 量化

Weight FP16 + KV Cache FP16

|模型|权重|KV Cache（tokens=2k）（batch=8）|KV Cache（tokens=8k）（batch=8）|KV Cache（tokens=32k）（batch=8）|
|-|-|-|-|-|
|Llama 7B| 14GB| 8GB|32GB|128GB|
|Llama 70B| 14GB| 5GB| 20GB|80GB|

Weight INT4 + KV Cache INT8

|模型 | 权重 | KV Cache（tokens=2k）（batch=8）| KV Cache（tokens=8k）（batch=8）| KV Cache（tokens=32k）（batch=8）|
|-|-|-|-|-|
|Llama 7B| 3.5GB| 4GB|16GB|64GB|
|Llama 70B| 35GB| 2.5GB| 10GB|40GB|

#### Weight Only 量化

**两个基本概念**
- 计算密集（compute-bound）:推理的绝大部分时间消耗在**数值计算**上；针对计算密集场景，可以通过使用更快的**硬件计算单元**来提升计算速度，比如量化为 `W8A8` 使用 INT8 Tensor Core 来加速计算。’
- 访存密集（memory-bound）:推理时，绝大部分时间消耗在**数据读取**上；针对访存密集型场景，一般是通过提高计算访存比来提升性能。

**LLM 是典型的访存密集型任务**
- 常见的 LLM 模型是 Decoder Only 架构。推理时大部分时间消耗在逐Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。



如图，A100 的 FP16 峰值算力为 312 TFLOPS，只有在 Batch Size 达到128 这个量级时，计算才成为推理的瓶颈，但由于 LLM 模型本身就很大
推理时的 KV Cache 也会占用很多显存，还有一些其他的因素影响（如Persistent Batch），实际推理时很难做到 128 这么大的 Batch Size。
![image](https://github.com/lvoooo/internLM-learning/assets/16740247/c32f3525-6f83-4826-996e-c396a1d9d953)

- 4bit Weight Only 量化，将 FP16 的模型权重量化为INT4，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本，提高了 Decoding 的速度。
- 加速的同时还节省了显存，同样的设备能够支持更大的模型以及更长的对话长度

访存密集型任务 ???

**如何做**

- LMDeploy 使用 MIT HAN LAB 开源的 AWQ 算法，量化为 4bit 模型；
- 推理时，`先把 4bit 权重，反量化回 FP16`（在 Kernel内部进行，从 Global Memory 读取时仍是 4bit），依旧使用的是 FP16 计算
相较于社区使用比较多的 GPTQ 算法，AWQ 的推理速度更快，量化的时间更短

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/1b4803a1-dfd1-4bf7-8f11-9b1f790730b6)


反量化不需要时间？

### TurboMind
- 持续批处理:  令牌桶？
- 高性能 cuda kernel：
- 有状态推理：服务端缓存？
- Blocked k/v cache： 缓存算法？

#### Blocked k/v cach

BlockSize = 2 X Layers X Heads X HeadDim X Seq X B；
Seq: 1 个 block 里的序列长度，默认128；
B：k/v数值精度对应的字节数；

llama-7b，2K序列长度，k/v block 内存1G

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/b31428a4-ac59-46ca-8adf-7da5be7a9b72)

Block 状态

- Free 未被任何序列占用
- Activate  正在被推理的序列占用
- Cache 被缓存中的序列占用

Block状态迁移

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/62cb54ea-0871-46b3-9665-4b8012e4a8c5)

#### 高性能 cuda kernel

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/cb25defb-cd8c-4c27-910b-494f496b7df6)

### API Server



