## 模型量化

-  降低显存
- 计算变快？
  
### KV Cache 量化

第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。

- 对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 (num_heads, head_dim) 的矩阵。
- 对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 (hidden_dim, ) 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。

```shell
# 计算 minmax
lmdeploy lite calibrate --model  /root/share/temp/model_repos/internlm-chat-7b/  --calib_dataset "c4"  --calib_samples 128  --calib_seqlen 2048  --work_dir ./quant_output
```
- 数据集选择 C4 
- 128 条输入样本
- 样本长度为 2048

第二步：通过 minmax 获取量化参数。

- 获取每一层的 K V 中心值（zp）和缩放值（scale）
- 对历史的 K 和 V 存储 quant 后的值，使用时在 dequant
- 
```shell
zp = (min+max) / 2
scale = (max-min) / 255
quant: q = round( (f-zp) / scale)
dequant: f = q * scale + zp
```

命令
```shell
lmdeploy lite kv_qparams \
  --work_dir ./quant_output  \
  --turbomind_dir workspace/triton_models/weights/ \
  --kv_sym False \
  --num_tp 1
```

kv_sym： True 则使用对称量化
num_tp：Tensor的并行数


```shell
(lmdeploy) root@intern-studio:~# lmdeploy lite kv_qparams \
>   --work_dir ./quant_output  \
>   --turbomind_dir workspace/triton_models/weights/ \
>   --kv_sym False \
>   --num_tp 1
Layer 0 MP 0 qparam:    0.06256103515625        -0.626953125    0.00891876220703125     0.029296875
Layer 1 MP 0 qparam:    0.0892333984375         -1.2109375      0.0207672119140625      -0.1923828125
Layer 2 MP 0 qparam:    0.0771484375    -0.015625       0.03631591796875        -0.8349609375
Layer 3 MP 0 qparam:    0.09783935546875        0.0390625       0.0296173095703125      -0.955078125
Layer 4 MP 0 qparam:    0.11334228515625        0.0625  0.03228759765625        0.244140625
Layer 5 MP 0 qparam:    0.1180419921875         0.16015625      0.050994873046875       1.251953125
Layer 6 MP 0 qparam:    0.1226806640625         -0.00390625     0.0306549072265625      0.275390625
Layer 7 MP 0 qparam:    0.134033203125  -0.7421875      0.03826904296875        -0.2265625
Layer 8 MP 0 qparam:    0.135009765625  -0.25   0.03753662109375        -1.0791015625
Layer 9 MP 0 qparam:    0.136474609375  -0.078125       0.037567138671875       0.544921875
Layer 10 MP 0 qparam:   0.1302490234375         -1.39453125     0.0265960693359375      -0.0458984375
Layer 11 MP 0 qparam:   0.1317138671875         0.359375        0.03216552734375        0.2529296875
Layer 12 MP 0 qparam:   0.131103515625  0.2734375       0.030731201171875       -0.0654296875
Layer 13 MP 0 qparam:   0.1357421875    0.1640625       0.0350341796875         0.044921875
Layer 14 MP 0 qparam:   0.127197265625  -0.2734375      0.030731201171875       0.0546875
Layer 15 MP 0 qparam:   0.131103515625  0.0703125       0.0345458984375         -0.40625
Layer 16 MP 0 qparam:   0.13232421875   -1.1015625      0.038177490234375       0.373046875
Layer 17 MP 0 qparam:   0.1287841796875         0.70703125      0.04364013671875        -0.48046875
Layer 18 MP 0 qparam:   0.1270751953125         -0.28125        0.04669189453125        -0.0390625
Layer 19 MP 0 qparam:   0.13916015625   -0.0703125      0.053680419921875       -0.39453125
Layer 20 MP 0 qparam:   0.154296875     -0.796875       0.04779052734375        -0.1171875
Layer 21 MP 0 qparam:   0.134521484375  0.0703125       0.051025390625  -1.17578125
Layer 22 MP 0 qparam:   0.1405029296875         -0.28125        0.053131103515625       -0.345703125
Layer 23 MP 0 qparam:   0.130615234375  -0.015625       0.0941162109375         -0.41796875
Layer 24 MP 0 qparam:   0.125732421875  -0.3203125      0.048980712890625       0.16015625
Layer 25 MP 0 qparam:   0.1390380859375         0.015625        0.06494140625   -0.125
Layer 26 MP 0 qparam:   0.1328125       -0.8828125      0.087646484375  -0.15234375
Layer 27 MP 0 qparam:   0.13427734375   -0.1015625      0.066650390625  0.1484375
Layer 28 MP 0 qparam:   0.1416015625    0.25    0.07989501953125        -0.6484375
Layer 29 MP 0 qparam:   0.1405029296875         0.484375        0.0819091796875         0.984375
Layer 30 MP 0 qparam:   0.135009765625  -0.5546875      0.0869140625    -0.35546875
Layer 31 MP 0 qparam:   0.133056640625  0.7109375       0.1517333984375         -0.25
```

第三步：修改配置 weights/config.ini 设置 `quant_policy` 为 4

> 如果用的是 TurboMind1.0，还需要修改参数 use_context_fmha，将其改为 0


#### 量化效果

量化前后的显存对比

|batch_size|	fp16 memory(MiB)	|int8 memory(MiB)	|diff(MiB)|
| ---------- | ---------------- | ---------------- | --------- |
| 8          | 22337            | 18241            | -4096     |
| 16         | 30593            | 22369            | -8224     |
| 32         | 47073            | 30625            | -16448    |
| 48         | 63553            | 38881            | -24672    |

量化前后的精准度（Accuracy）对比

| task          | dataset         | metric        | int8  | fp16  | diff  |
| ------------- | --------------- | ------------- | ----- | ----- | ----- |
| Language      | winogrande      | accuracy      | 60.77 | 61.48 | -0.71 |
| Knowledge     | nq              | score         | 2.69  | 2.60  | +0.09 |
| Reasoning     | gsm8k           | accuracy      | 33.28 | 34.72 | -1.44 |
| Reasoning     | bbh             | naive_average | 20.12 | 20.51 | -0.39 |
| Understanding | openbookqa_fact | accuracy      | 82.40 | 82.20 | +0.20 |
| Understanding | eprstmt-dev     | accuracy      | 90.62 | 88.75 | +1.87 |
| Safety        | crows_pairs     | accuracy      | 32.56 | 31.43 | +1.13 |

### W4A16 量化

W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。

第一步：同之前；
第二部：量化权重模型；
```shell
# 量化权重模型
lmdeploy lite auto_awq \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --w_bits 4 \
  --w_group_size 128 \
  --work_dir ./quant_output 
```
w_bits：表示量化的位数
w_group_size： 表示量化分组统计的尺寸

> 实际存储时，8个 4bit 权重会被打包到一个 int32 值中

```shell
# 转换模型的layout，存放在默认路径 ./workspace 下
lmdeploy convert  internlm-chat-7b ./quant_output \
    --model-format awq \
    --group-size 128 \
    --dst_path ./workspace_quant
```

group-size 就是上一步的那个 w_group_size

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/4bfa0890-e8f0-49e4-ae76-e12f8a760fb7)


#### 量化效果
-  RTX 4090 
- 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 
- BatchSize = 1，prompt_tokens=1，completion_tokens=512

| model            | llm-awq | mlc-llm | turbomind |
| ---------------- | ------- | ------- | --------- |
| Llama-2-7B-chat  | 112.9   | 159.4   | 206.4     |
| Llama-2-13B-chat | N/A     | 90.7    | 115.8     |

不同精度和上下文长度下

| model(context length) | 16bit(2048) | 4bit(2048) | 16bit(4096) | 4bit(4096) |
| --------------------- | ----------- | ---------- | ----------- | ---------- |
| Llama-2-7B-chat       | 15.1        | 6.3        | 16.2        | 7.5        |
| Llama-2-13B-chat      | OOM         | 10.3       | OOM         | 12.0       |

### 最佳实践

考虑框架、显卡、量化版本、功能开关；

在各种配置下尝试，看效果能否满足需要。

![image](https://github.com/lvoooo/internLM-learning/assets/16740247/4220ed0a-a5d4-4cd1-a88c-617b433c667c)

根据实践经验，一般情况下：

- 精度越高，显存占用越多，推理效率越低，但一般效果较好。
- Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。
- 端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。

Demo：
-  如果资源足够（有GPU卡很重要），那就用非量化的正常版本。
- 如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。
- 如果生成文本长度很长，显存不够，就开启 KV Cache。
